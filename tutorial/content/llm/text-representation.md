# 文本的数学表示与编码

## 从文字到数字：计算机如何理解语言？

计算机无法直接理解文字，就像你无法直接理解一串乱码。为了让计算机处理语言，我们需要把文字转换成数字。这个过程就像翻译——把"人类语言"翻译成"机器语言"。

让我们用一个例子贯穿本章：**"今天天气真好，我想出去玩"**。

## 第一步：分词——把句子切成小块

### 什么是分词？

分词（Tokenization）就是把连续的文本切分成更小的单位，这些单位叫做"token"（词元）。

> **直观理解**：分词就像把一篇文章拆成单词。当你读英文时，你会自然地按空格把句子分成单词；读中文时，你会按词语来理解，而不是按单个字。

### 不同的分词方法

#### 方法1：按字符分词

最简单的方法是按字符切分：

```
"今天天气真好，我想出去玩"
→ ['今', '天', '天', '气', '真', '好', '，', '我', '想', '出', '去', '玩']
```

**优点**：词汇表很小，不会有"未知词"
**缺点**：序列太长，缺乏语义信息

#### 方法2：按词语分词

按词语切分：

```
"今天天气真好，我想出去玩"
→ ['今天', '天气', '真好', '，', '我', '想', '出去', '玩']
```

**优点**：语义单位相对完整
**缺点**：词汇表很大，生僻词会变成"未知词"

#### 方法3：子词分词（现代LLM的标准）

这是现代大语言模型使用的方法，在字符和词语之间取得平衡。

> **直观理解**：子词分词就像搭积木。开始有很多小积木（字符），我们不断把经常一起出现的小积木粘成大积木（子词），最后用这些大积木来表示文本。

比如，经过子词分词后，我们的例子可能变成：

```
"今天天气真好，我想出去玩"
→ ['今天', '天气', '真', '好', '，', '我', '想', '出', '去', '玩']
```

注意："真好"被拆成了"真"和"玩"，因为"真"和"好"可能在其他地方也经常单独出现。

### 子词分词的工作原理：字节对编码（BPE）

BPE（Byte Pair Encoding）是一种聪明的算法，它的核心思想是：**不断合并最常出现的字符对**。

让我们用一个简单的英文例子来说明：

假设我们有很多这样的句子：
```
"low low low low lowest lowest newer newer newer newer newer wider wider wider"
```

**初始状态**（每个字符都是独立的token）：
```
词汇表: {'l', 'o', 'w', 'e', 'r', 'n', 'i', 'd', ' ', 's', 't'}
```

**第一次迭代**：发现"e"和"r"经常一起出现，合并成"er"
```
"low low low low low er st low er st n e w er n e w er n e w er n e w er wid er wid er wid er"
```

**第二次迭代**：发现"l"和"o"经常一起出现，合并成"lo"
```
"lo w lo w lo w lo w lo er st lo er st n e w er n e w er n e w er n e w er wid er wid er wid er"
```

**第三次迭代**：发现"lo"和"w"经常一起出现，合并成"low"
```
"low low low low low er st low er st n e w er n e w er n e w er n e w er wid er wid er wid er"
```

继续迭代，最终得到：
```
词汇表: {'low', 'er', 'st', 'new', 'wid', ' ', ...}
```

这样，"lowest"就变成了['low', 'er', 'st']，而不是单个字符。

> **为什么这样做？** 因为"low"、"er"、"st"这些子词在很多其他词中也会用到，比如"lower"、"newer"、"wider"。这样既控制了词汇表大小，又保留了语义信息。

## 第二步：词表——给每个token分配一个数字

分词后，我们需要给每个token分配一个唯一的数字，就像给每个人发身份证号。

### 词表的结构

一个典型的词表可能长这样：

```
{
  '<pad>': 0,      # 填充符，用于对齐不同长度的句子
  '<unk>': 1,      # 未知词，表示词表中没有的词
  '<bos>': 2,      # 句子开始
  '<eos>': 3,      # 句子结束
  '，': 4,
  '。': 5,
  '我': 6,
  '想': 7,
  '今天': 8,
  '天气': 9,
  '真': 10,
  '好': 11,
  '出': 12,
  '去': 13,
  '玩': 14,
  ...
}
```

### 特殊token的作用

| Token | 用途 | 直观理解 |
|-------|------|----------|
| `<pad>` | 填充序列到相同长度 | 就像在试卷上填空，让所有句子长度一样 |
| `<unk>` | 表示词表外的token | 就像"未知号码"，表示遇到了不认识的人 |
| `<bos>` | 标记序列开始 | 就像"开始"的信号 |
| `<eos>` | 标记序列结束 | 就像"结束"的信号 |

### 把我们的例子转换成数字

假设我们的词表包含上面这些词，那么：

```
"今天天气真好，我想出去玩"
分词后：['今天', '天气', '真', '好', '，', '我', '想', '出', '去', '玩']
转换后：[8, 9, 10, 11, 4, 6, 7, 12, 13, 14]
```

现在，我们的句子变成了一串数字！

## 第三步：词嵌入——把数字变成有意义的向量

数字本身没有语义含义。数字8和数字9之间有什么关系？不知道。为了让计算机理解语义，我们需要把数字转换成**向量**。

### 什么是向量？

向量就是一个有序的数字列表，比如：

```
[0.1, -0.2, 0.5, 0.8, -0.3]
```

你可以把向量想象成空间中的一个点，或者一个箭头。

> **直观理解**：向量就像一个人的"特征描述"。比如描述一个人，你可能会说：身高175cm，体重65kg，年龄25岁。这些数字组合起来就是这个人的"向量"。

### 词嵌入的神奇之处

词嵌入把每个词转换成一个固定长度的向量，比如512维或768维。神奇的是，**语义相似的词在向量空间中距离更近**。

举个例子：
- "医生"的向量可能和"护士"的向量很近
- "苹果"的向量可能和"香蕉"的向量很近
- "开心"的向量可能和"快乐"的向量很近

> **经典的例子**：在向量空间中，有这样的关系：
> `King（国王） - Man（男人） + Woman（女人） ≈ Queen（女王）`

这意味着，从"国王"到"女王"的"方向"，和从"男人"到"女人"的"方向"是相似的！

### 词嵌入是如何学习的？

词嵌入不是人工设计的，而是模型在训练过程中自动学习的。

> **直观理解**：想象你在读很多书，你会注意到"医生"经常和"医院"、"病人"、"治疗"一起出现，而"厨师"经常和"厨房"、"菜谱"、"烹饪"一起出现。久而久之，你的大脑就会把"医生"和"医院"关联起来，把"厨师"和"厨房"关联起来。词嵌入就是让计算机做同样的事情。

当模型在海量文本上训练时，它会学习到：
- "今天"和"昨天"、"明天"经常一起出现
- "天气"和"晴朗"、"阴雨"经常一起出现
- "玩"和"开心"、"快乐"经常一起出现

这些共现关系会被编码到向量中。

### 把我们的例子转换成向量

假设我们使用768维的词嵌入，那么：

```
"今天天气真好，我想出去玩"
数字序列：[8, 9, 10, 11, 4, 6, 7, 12, 13, 14]
向量序列：
[
  [0.1, -0.2, 0.5, ...],  # "今天"的768维向量
  [0.3, 0.1, -0.4, ...],  # "天气"的768维向量
  [-0.1, 0.2, 0.6, ...],  # "真"的768维向量
  [0.4, -0.1, 0.3, ...],  # "好"的768维向量
  ...
]
```

现在，每个词都变成了一个有意义的向量！

## 第四步：位置编码——告诉模型词的顺序

有一个问题：向量本身不包含位置信息。如果我们把"我想出去玩"和"玩出去我想"转换成向量，模型无法知道它们的区别。

> **直观理解**：就像你拿到一副洗乱的扑克牌，你知道每张牌是什么，但不知道它们原本的顺序。位置编码就是给每张牌标上"第1张"、"第2张"...

### 位置编码的方法

最简单的方法是给每个位置分配一个固定的向量：

```
位置0：[0.0, 0.0, 0.0, ...]
位置1：[0.1, 0.1, 0.1, ...]
位置2：[0.2, 0.2, 0.2, ...]
...
```

但现代大语言模型使用更聪明的方法——**正弦位置编码**。

> **直观理解**：正弦位置编码就像给每个位置一个独特的"指纹"。不同位置使用不同频率的正弦波，就像音乐中不同的音符。

### 把位置信息加到词向量上

位置编码很简单：**把位置向量加到词向量上**。

```
最终表示 = 词向量 + 位置向量
```

对于我们的例子：

```
"今天天气真好，我想出去玩"
词向量序列：
[
  [0.1, -0.2, 0.5, ...],  # "今天"
  [0.3, 0.1, -0.4, ...],  # "天气"
  ...
]

位置向量序列：
[
  [0.0, 0.1, 0.0, ...],  # 位置0
  [0.1, 0.2, 0.1, ...],  # 位置1
  ...
]

最终表示：
[
  [0.1, -0.1, 0.5, ...],  # "今天" + 位置0
  [0.4, 0.3, -0.3, ...],  # "天气" + 位置1
  ...
]
```

现在，模型不仅知道每个词是什么，还知道它在句子的哪个位置！

## 完整流程总结

让我们把整个过程串起来：

```mermaid
graph LR
    A[原始文本] --> B[分词]
    B --> C[Token序列]
    C --> D[词表映射]
    D --> E[数字序列]
    E --> F[词嵌入]
    F --> G[向量序列]
    G --> H[位置编码]
    H --> I[最终表示]
```

对于我们的例子：

```
原始文本："今天天气真好，我想出去玩"
    ↓
分词：['今天', '天气', '真', '好', '，', '我', '想', '出', '去', '玩']
    ↓
词表映射：[8, 9, 10, 11, 4, 6, 7, 12, 13, 14]
    ↓
词嵌入：[[向量1], [向量2], [向量3], ...]
    ↓
位置编码：[[向量1+位置0], [向量2+位置1], [向量3+位置2], ...]
    ↓
最终表示：计算机可以处理的数值矩阵
```

## 为什么要这样做？

你可能会问：为什么要这么麻烦？直接用文字不行吗？

答案是：**计算机只能处理数字**。通过这些步骤，我们：
1. 把离散的文字转换成连续的数字（词嵌入）
2. 让相似的词在数学上"相似"（向量空间）
3. 保留词的顺序信息（位置编码）

这样，计算机就可以用数学运算来处理语言了！

## 小结

本章我们学习了如何把文本转换成计算机能理解的形式：

1. **分词**：把句子切成token，使用子词分词在效率和语义之间取得平衡
2. **词表**：给每个token分配一个唯一的数字
3. **词嵌入**：把数字转换成向量，让语义相似的词在向量空间中距离更近
4. **位置编码**：给向量加上位置信息，让模型知道词的顺序

现在，我们的句子已经变成了一个数值矩阵，准备好进入模型的下一层了。下一章，我们将学习大语言模型的核心——注意力机制。
